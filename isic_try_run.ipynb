{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1870628d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d7203",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install hdf5plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34718240",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c4f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da967ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clearml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efc46a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd49134",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966b95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install medpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63023c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydicom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076f13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vit_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77671dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980b91e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Documents\\fuse-med-ml-master-1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(r'C:\\Users\\USER\\Documents\\fuse-med-ml-master-1'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "print (module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c387068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ISIC\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "(C) Copyright 2021 IBM Corp.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "   http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n",
    "\n",
    "Created on June 30, 2021\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import logging\n",
    "from typing import OrderedDict, Sequence, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from fuse.dl.models import ModelMultiHead\n",
    "from fuse.dl.models.backbones.backbone_resnet import BackboneResnet\n",
    "from fuse.dl.models.heads.head_global_pooling_classifier import HeadGlobalPoolingClassifier\n",
    "from fuse.dl.models.backbones.backbone_inception_resnet_v2 import BackboneInceptionResnetV2\n",
    "\n",
    "from fuse.eval.metrics.classification.metrics_thresholding_common import MetricApplyThresholds\n",
    "from fuse.eval.metrics.classification.metrics_classification_common import MetricAccuracy, MetricAUCROC, MetricROCCurve\n",
    "\n",
    "from fuse.utils.utils_debug import FuseDebug\n",
    "from fuse.utils.utils_logger import fuse_logger_start\n",
    "from fuse.utils.file_io.file_io import create_dir, save_dataframe, load_pickle\n",
    "import fuse.utils.gpu as GPU\n",
    "\n",
    "from fuse.eval.evaluator import EvaluatorDefault\n",
    "from fuse.dl.losses.loss_default import LossDefault\n",
    "from fuse.data.utils.collates import CollateDefault\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from fuse.dl.lightning.pl_module import LightningModuleDefault\n",
    "from fuse.dl.lightning.pl_funcs import convert_predictions_to_dataframe\n",
    "from pytorch_lightning.utilities.rank_zero import rank_zero_only\n",
    "\n",
    "from fuseimg.datasets.isic import ISIC, ISICDataModule\n",
    "###Michal Add examples to the path (before fuse_examples)\n",
    "from examples.fuse_examples.imaging.classification.isic.golden_members import FULL_GOLDEN_MEMBERS, TEN_GOLDEN_MEMBERS\n",
    "\n",
    "import torch.nn as nn\n",
    "from fuse.dl.models.model_wrapper import ModelWrapSeqToDict\n",
    "from fuse.dl.models.backbones.backbone_vit import ViT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0f91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################################\n",
    "# Fuse\n",
    "###########################################################################################################\n",
    "##########################################\n",
    "# Debug modes\n",
    "##########################################\n",
    "###Michal change to ndebug \n",
    "mode = \"default\"  # Options: '', 'debug'. See details in FuseDebug\n",
    "debug = FuseDebug(mode)\n",
    "\n",
    "##########################################\n",
    "# GPUs and Workers\n",
    "##########################################\n",
    "###Michal change GPUs from 1 to 0 and back to 1 (for cpu)\n",
    "NUM_GPUS = 0 # supports multiple gpu training with DDP strategy\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "##########################################\n",
    "# Modality\n",
    "##########################################\n",
    "multimodality = False  # Set: 'False' to use only imaging, 'True' to use imaging & meta-data\n",
    "\n",
    "##########################################\n",
    "# Model Type\n",
    "##########################################\n",
    "model_type = \"CNN\"  # Set: 'Transformer' to use ViT/MMViT, 'CNN' to use InceptionResNet\n",
    "\n",
    "##########################################\n",
    "# Output Paths\n",
    "##########################################\n",
    "\n",
    "\n",
    "ROOT = \"./_examples/isic/\"\n",
    "DATA = os.environ[\"ISIC19_DATA_PATH\"] if \"ISIC19_DATA_PATH\" in os.environ else os.path.join(ROOT, \"data_dir\")\n",
    "modality = \"multimodality\" if multimodality else \"imaging\"\n",
    "model_dir = os.path.join(ROOT, f\"model_dir_{modality}\")\n",
    "PATHS = {\n",
    "    \"model_dir\": model_dir,\n",
    "    \"inference_dir\": os.path.join(model_dir, \"infer_dir\"),\n",
    "    \"eval_dir\": os.path.join(model_dir, \"eval_dir\"),\n",
    "    \"data_dir\": DATA,\n",
    "    \"cache_dir\": os.path.join(ROOT, \"cache_dir\"),\n",
    "    \"data_split_filename\": os.path.join(model_dir, \"isic_split.pkl\"),\n",
    "}\n",
    "\n",
    "\n",
    "##########################################\n",
    "# Train Common Params\n",
    "##########################################\n",
    "TRAIN_COMMON_PARAMS = {}\n",
    "# ============\n",
    "# Data\n",
    "# ============\n",
    "TRAIN_COMMON_PARAMS[\"data.batch_size\"] = 32  # effective batch size = batch_size * num_gpus\n",
    "TRAIN_COMMON_PARAMS[\"data.num_workers\"] = NUM_WORKERS\n",
    "TRAIN_COMMON_PARAMS[\"data.num_folds\"] = 5\n",
    "TRAIN_COMMON_PARAMS[\"data.train_folds\"] = [0, 1, 2]\n",
    "TRAIN_COMMON_PARAMS[\"data.validation_folds\"] = [3]\n",
    "TRAIN_COMMON_PARAMS[\"data.infer_folds\"] = [4]\n",
    "TRAIN_COMMON_PARAMS[\"data.samples_ids\"] = {\"all\": None, \"golden\": THREE_GOLDEN_MEMBERS}[\"golden\"]\n",
    "\n",
    "\n",
    "# ===============\n",
    "# PL Trainer\n",
    "# ===============\n",
    "TRAIN_COMMON_PARAMS[\"trainer.num_epochs\"] = 30\n",
    "TRAIN_COMMON_PARAMS[\"trainer.num_devices\"] = NUM_GPUS\n",
    "###Michal\n",
    "TRAIN_COMMON_PARAMS[\"trainer.accelerator\"] = \"cpu\"\n",
    "TRAIN_COMMON_PARAMS[\"trainer.strategy\"] = \"ddp\" if NUM_GPUS > 1 else None\n",
    "\n",
    "# ===============\n",
    "# Optimizer\n",
    "# ===============\n",
    "TRAIN_COMMON_PARAMS[\"opt.lr\"] = 1e-5\n",
    "TRAIN_COMMON_PARAMS[\"opt.weight_decay\"] = 1e-3\n",
    "\n",
    "# ===============\n",
    "# Model\n",
    "# ===============\n",
    "if model_type == \"CNN\":\n",
    "    TRAIN_COMMON_PARAMS[\"model\"] = dict(\n",
    "        dropout_rate=0.5,\n",
    "        layers_description=(256,),\n",
    "        tabular_data_inputs=[(\"data.input.clinical.all\", 19)] if multimodality else None,\n",
    "        tabular_layers_description=(128,) if multimodality else tuple(),\n",
    "    )\n",
    "elif model_type == \"Transformer\":\n",
    "    token_dim = 768\n",
    "    TRAIN_COMMON_PARAMS[\"model\"] = dict(\n",
    "        token_dim=token_dim,\n",
    "        projection_kwargs=dict(image_shape=[300, 300], patch_shape=[30, 30], channels=3),\n",
    "        transformer_kwargs=dict(depth=12, heads=12, mlp_dim=token_dim * 4, dim_head=64, dropout=0.0, emb_dropout=0.0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d8fc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data.batch_size': 32, 'data.num_workers': 0, 'data.num_folds': 5, 'data.train_folds': [0, 1, 2], 'data.validation_folds': [3], 'data.infer_folds': [4], 'data.samples_ids': ['ISIC_0072637', 'ISIC_0072638', 'ISIC_0072639', 'ISIC_0072640', 'ISIC_0072641', 'ISIC_0072642', 'ISIC_0072646', 'ISIC_0072647', 'ISIC_0072648', 'ISIC_0072649'], 'trainer.num_epochs': 30, 'trainer.num_devices': 0, 'trainer.accelerator': 'cpu', 'trainer.strategy': None, 'opt.lr': 1e-05, 'opt.weight_decay': 0.001, 'model': {'dropout_rate': 0.5, 'layers_description': (256,), 'tabular_data_inputs': None, 'tabular_layers_description': ()}}\n"
     ]
    }
   ],
   "source": [
    "print (TRAIN_COMMON_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6167e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_softmax(logits: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    cls_preds = F.softmax(logits, dim=1)\n",
    "    return logits, cls_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbbe5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMViT(ViT):\n",
    "    def __init__(self, token_dim: int, projection_kwargs: dict, transformer_kwargs: dict, multimodality: bool):\n",
    "        super().__init__(token_dim, projection_kwargs, transformer_kwargs)\n",
    "        self.multimodality = multimodality\n",
    "        num_tokens = self.projection_layer.num_tokens\n",
    "        self.token_dim = token_dim\n",
    "        self._head = nn.Linear(token_dim, 8)\n",
    "        if self.multimodality:\n",
    "            # change pos embedding to accept additional token for multimodal\n",
    "            self.transformer.pos_embedding = nn.Parameter(torch.randn(1, num_tokens + 2, token_dim))\n",
    "\n",
    "    # This forward can be Multimodal or just Imaging\n",
    "    def forward(self, img_x: torch.Tensor, clinical_x: torch.Tensor = None) -> torch.Tensor:\n",
    "        img_x = self.projection_layer(img_x)\n",
    "        if self.multimodality:\n",
    "            clinical_x = clinical_x.unsqueeze(1)\n",
    "            clinical_x_zeros = torch.zeros((img_x.shape[0], 1, self.token_dim))\n",
    "            clinical_x_zeros[:, :, :19] = clinical_x\n",
    "            clinical_x = clinical_x_zeros.cuda()\n",
    "            x = torch.cat((img_x, clinical_x), 1)\n",
    "        else:\n",
    "            x = img_x\n",
    "        x = self.transformer(x)\n",
    "        x = self._head(x[:, 0])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30c0fa52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_transformer_model(\n",
    "    token_dim: int,\n",
    "    projection_kwargs: dict,\n",
    "    transformer_kwargs: dict,\n",
    ") -> ModelWrapSeqToDict:\n",
    "    torch_model = MMViT(\n",
    "        token_dim=token_dim,\n",
    "        projection_kwargs=projection_kwargs,\n",
    "        transformer_kwargs=transformer_kwargs,\n",
    "        multimodality=multimodality,\n",
    "    )\n",
    "    model = ModelWrapSeqToDict(\n",
    "        model=torch_model,\n",
    "        model_inputs=[\"data.input.img\", \"data.input.clinical.all\"] if multimodality else [\"data.input.img\"],\n",
    "        post_forward_processing_function=perform_softmax,\n",
    "        model_outputs=[\"model.logits.head_0\", \"model.output.head_0\"],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1014d916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(\n",
    "    dropout_rate: float,\n",
    "    layers_description: Sequence[int],\n",
    "    tabular_data_inputs: Sequence[Tuple[str, int]],\n",
    "    tabular_layers_description: Sequence[int],\n",
    ") -> torch.nn.Module:\n",
    "    \"\"\"\n",
    "    creates the model\n",
    "    \"\"\"\n",
    "    print (\"before ModelMultiHead\")\n",
    "    model = ModelMultiHead(\n",
    "        conv_inputs=((\"data.input.img\", 3),),\n",
    "        backbone={\n",
    "            \"Resnet18\": BackboneResnet(pretrained=True, in_channels=3, name=\"resnet18\"),\n",
    "            \"InceptionResnetV2\": BackboneInceptionResnetV2(input_channels_num=3, logical_units_num=43),\n",
    "        }[\"InceptionResnetV2\"],\n",
    "        heads=[\n",
    "            HeadGlobalPoolingClassifier(\n",
    "                head_name=\"head_0\",\n",
    "                dropout_rate=dropout_rate,\n",
    "                conv_inputs=[(\"model.backbone_features\", 1536)],\n",
    "                tabular_data_inputs=tabular_data_inputs,\n",
    "                layers_description=layers_description,\n",
    "                tabular_layers_description=tabular_layers_description,\n",
    "                num_classes=8,\n",
    "                pooling=\"avg\",\n",
    "            ),\n",
    "        ],\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57e7eb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_dir': './_examples/isic/model_dir_imaging', 'inference_dir': './_examples/isic/model_dir_imaging\\\\infer_dir', 'eval_dir': './_examples/isic/model_dir_imaging\\\\eval_dir', 'data_dir': './_examples/isic/data_dir', 'cache_dir': './_examples/isic/cache_dir', 'data_split_filename': './_examples/isic/model_dir_imaging\\\\isic_split.pkl'}\n"
     ]
    }
   ],
   "source": [
    "print (PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdaa302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_datamodule(paths: dict, train_common_params: dict) -> pl.LightningDataModule:\n",
    "    \"\"\"\n",
    "    In order to support the DDP strategy one need to create a Lightning Data Module.\n",
    "    \"\"\"\n",
    "    print (\"before ISICDataModule\")\n",
    "    datamodule = ISICDataModule(\n",
    "        data_dir=paths[\"data_dir\"],\n",
    "        cache_dir=paths[\"cache_dir\"],\n",
    "        num_workers=train_common_params[\"data.num_workers\"],\n",
    "        batch_size=train_common_params[\"data.batch_size\"],\n",
    "        train_folds=train_common_params[\"data.train_folds\"],\n",
    "        validation_folds=train_common_params[\"data.validation_folds\"],\n",
    "        infer_folds=train_common_params[\"data.infer_folds\"],\n",
    "        split_filename=paths[\"data_split_filename\"],\n",
    "        sample_ids=train_common_params[\"data.samples_ids\"],\n",
    "        reset_cache=False,\n",
    "        reset_split=False,\n",
    "        use_batch_sampler=True if NUM_GPUS <= 1 else False,\n",
    "    )\n",
    "\n",
    "    return datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc2376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# Train Template\n",
    "#################################\n",
    "def run_train(paths: dict, train_common_params: dict) -> None:\n",
    "    # ==============================================================================\n",
    "    # Logger\n",
    "    # ==============================================================================\n",
    "    fuse_logger_start(output_path=paths[\"model_dir\"], console_verbose_level=logging.INFO)\n",
    "    lgr = logging.getLogger(\"Fuse\")\n",
    "    lgr.info(\"Fuse Train\", {\"attrs\": [\"bold\", \"underline\"]})\n",
    "\n",
    "    lgr.info(f'model_dir={paths[\"model_dir\"]}', {\"color\": \"magenta\"})\n",
    "    lgr.info(f'data_dir={paths[\"data_dir\"]}', {\"color\": \"magenta\"})\n",
    "    lgr.info(f'cache_dir={paths[\"cache_dir\"]}', {\"color\": \"magenta\"})\n",
    "\n",
    "    # ==============================================================================\n",
    "    # Data\n",
    "    # ==============================================================================\n",
    "    lgr.info(\"Datamodule:\", {\"attrs\": \"bold\"})\n",
    "\n",
    "    datamodule = create_datamodule(paths, train_common_params)\n",
    "\n",
    "    lgr.info(\"Datamodule: Done\", {\"attrs\": \"bold\"})\n",
    "\n",
    "#     # ==============================================================================\n",
    "#     # Model\n",
    "#     # ==============================================================================\n",
    "#     lgr.info(\"Model:\", {\"attrs\": \"bold\"})\n",
    "\n",
    "#     if model_type == \"Transformer\":\n",
    "#         model = create_transformer_model(**train_common_params[\"model\"])\n",
    "#     elif model_type == \"CNN\":\n",
    "#         model = create_cnn_model(**train_common_params[\"model\"])\n",
    "\n",
    "#     lgr.info(\"Model: Done\", {\"attrs\": \"bold\"})\n",
    "\n",
    "#     # ====================================================================================\n",
    "#     #  Loss\n",
    "#     # ====================================================================================\n",
    "#     losses = {\n",
    "#         \"cls_loss\": LossDefault(pred=\"model.logits.head_0\", target=\"data.label\", callable=F.cross_entropy, weight=1.0),\n",
    "#     }\n",
    "\n",
    "#     # ====================================================================================\n",
    "#     # Metrics\n",
    "#     # ====================================================================================\n",
    "#     class_names = [\"MEL\", \"NV\", \"BCC\", \"AK\", \"BKL\", \"DF\", \"VASC\", \"SCC\"]\n",
    "#     train_metrics = OrderedDict(\n",
    "#         [\n",
    "#             (\"op\", MetricApplyThresholds(pred=\"model.output.head_0\")),  # will apply argmax\n",
    "#             (\"auc\", MetricAUCROC(pred=\"model.output.head_0\", target=\"data.label\", class_names=class_names)),\n",
    "#             (\"accuracy\", MetricAccuracy(pred=\"results:metrics.op.cls_pred\", target=\"data.label\")),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     validation_metrics = copy.deepcopy(train_metrics)  # use the same metrics in validation as well\n",
    "\n",
    "#     best_epoch_source = dict(monitor=\"validation.metrics.auc.macro_avg\", mode=\"max\")\n",
    "\n",
    "#     # create optimizer\n",
    "#     optimizer = optim.Adam(\n",
    "#         model.parameters(), lr=train_common_params[\"opt.lr\"], weight_decay=train_common_params[\"opt.weight_decay\"]\n",
    "#     )\n",
    "\n",
    "#     # create learning scheduler\n",
    "#     lr_scheduler = {\n",
    "#         \"ReduceLROnPlateau\": optim.lr_scheduler.ReduceLROnPlateau(optimizer),\n",
    "#         \"CosineAnnealing\": optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1),\n",
    "#     }[\"ReduceLROnPlateau\"]\n",
    "#     lr_sch_config = dict(scheduler=lr_scheduler, monitor=\"validation.losses.total_loss\")\n",
    "\n",
    "#     # optimizer and lr sch - see pl.LightningModule.configure_optimizers return value for all options\n",
    "#     optimizers_and_lr_schs = dict(optimizer=optimizer, lr_scheduler=lr_sch_config)\n",
    "\n",
    "    # =====================================================================================\n",
    "    #  Train\n",
    "    # =====================================================================================\n",
    "#     lgr.info(\"Train:\", {\"attrs\": \"bold\"})\n",
    "\n",
    "#     # create instance of PL module - FuseMedML generic version\n",
    "#     print (\"before LightningModuleDefault\")\n",
    "#     pl_module = LightningModuleDefault(\n",
    "#         model_dir=paths[\"model_dir\"],\n",
    "#         model=model,\n",
    "#         losses=losses,\n",
    "#         train_metrics=train_metrics,\n",
    "#         validation_metrics=validation_metrics,\n",
    "#         best_epoch_source=best_epoch_source,\n",
    "#         optimizers_and_lr_schs=optimizers_and_lr_schs,\n",
    "#     )\n",
    "\n",
    "#     # create lightning trainer.\n",
    "#     print (\"before pl.Trainer\")\n",
    "#     pl_trainer = pl.Trainer(\n",
    "#         default_root_dir=paths[\"model_dir\"],\n",
    "#         max_epochs=train_common_params[\"trainer.num_epochs\"],\n",
    "#         accelerator=train_common_params[\"trainer.accelerator\"],\n",
    "#         devices=train_common_params[\"trainer.num_devices\"],\n",
    "#         strategy=train_common_params[\"trainer.strategy\"],\n",
    "#     )\n",
    "\n",
    "#     # train\n",
    "#     print (\"before fit\")\n",
    "#     pl_trainer.fit(pl_module, datamodule=datamodule)\n",
    "\n",
    "#     lgr.info(\"Train: Done\", {\"attrs\": \"bold\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d4bd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Inference Common Params\n",
    "######################################\n",
    "INFER_COMMON_PARAMS = {}\n",
    "INFER_COMMON_PARAMS[\"infer_filename\"] = \"infer_file.gz\"\n",
    "INFER_COMMON_PARAMS[\"checkpoint\"] = \"best_epoch.ckpt\"\n",
    "INFER_COMMON_PARAMS[\"data.num_workers\"] = NUM_WORKERS\n",
    "INFER_COMMON_PARAMS[\"data.infer_folds\"] = [4]  # infer validation set\n",
    "INFER_COMMON_PARAMS[\"data.batch_size\"] = 4\n",
    "\n",
    "INFER_COMMON_PARAMS[\"model\"] = TRAIN_COMMON_PARAMS[\"model\"]\n",
    "INFER_COMMON_PARAMS[\"trainer.num_devices\"] = 1  # No need for multi-gpu in inference\n",
    "INFER_COMMON_PARAMS[\"trainer.accelerator\"] = TRAIN_COMMON_PARAMS[\"trainer.accelerator\"]\n",
    "\n",
    "######################################\n",
    "# Inference Template\n",
    "######################################\n",
    "\n",
    "\n",
    "@rank_zero_only\n",
    "def run_infer(paths: dict, infer_common_params: dict) -> None:\n",
    "    create_dir(paths[\"inference_dir\"])\n",
    "    infer_file = os.path.join(paths[\"inference_dir\"], infer_common_params[\"infer_filename\"])\n",
    "    checkpoint_file = os.path.join(paths[\"model_dir\"], infer_common_params[\"checkpoint\"])\n",
    "\n",
    "    ## Logger\n",
    "    fuse_logger_start(output_path=paths[\"inference_dir\"], console_verbose_level=logging.INFO)\n",
    "    lgr = logging.getLogger(\"Fuse\")\n",
    "    lgr.info(\"Fuse Inference\", {\"attrs\": [\"bold\", \"underline\"]})\n",
    "    lgr.info(f\"infer_filename={infer_file}\", {\"color\": \"magenta\"})\n",
    "\n",
    "    ## Data\n",
    "    folds = load_pickle(paths[\"data_split_filename\"])  # assume exists and created in train func\n",
    "\n",
    "    infer_sample_ids = []\n",
    "    for fold in infer_common_params[\"data.infer_folds\"]:\n",
    "        infer_sample_ids += folds[fold]\n",
    "\n",
    "    # Create dataset\n",
    "    infer_dataset = ISIC.dataset(paths[\"data_dir\"], paths[\"cache_dir\"], samples_ids=infer_sample_ids, train=False)\n",
    "\n",
    "    # dataloader\n",
    "    infer_dataloader = DataLoader(\n",
    "        dataset=infer_dataset,\n",
    "        collate_fn=CollateDefault(),\n",
    "        batch_size=infer_common_params[\"data.batch_size\"],\n",
    "        num_workers=infer_common_params[\"data.num_workers\"],\n",
    "    )\n",
    "\n",
    "    # load python lightning module\n",
    "    if model_type == \"Transformer\":\n",
    "        model = create_transformer_model(**infer_common_params[\"model\"])\n",
    "    elif model_type == \"CNN\":\n",
    "        model = create_cnn_model(**infer_common_params[\"model\"])\n",
    "\n",
    "    pl_module = LightningModuleDefault.load_from_checkpoint(\n",
    "        checkpoint_file, model_dir=paths[\"model_dir\"], model=model, map_location=\"cpu\", strict=True\n",
    "    )\n",
    "    # set the prediction keys to extract (the ones used be the evaluation function).\n",
    "    pl_module.set_predictions_keys([\"model.output.head_0\", \"data.label\"])  # which keys to extract and dump into file\n",
    "\n",
    "    # create a trainer instance\n",
    "    pl_trainer = pl.Trainer(\n",
    "        default_root_dir=paths[\"model_dir\"],\n",
    "        accelerator=infer_common_params[\"trainer.accelerator\"],\n",
    "        devices=infer_common_params[\"trainer.num_devices\"],\n",
    "        auto_select_gpus=True,\n",
    "        max_epochs=0,\n",
    "    )\n",
    "    predictions = pl_trainer.predict(pl_module, infer_dataloader, return_predictions=True)\n",
    "\n",
    "    # convert list of batch outputs into a dataframe\n",
    "    infer_df = convert_predictions_to_dataframe(predictions)\n",
    "    save_dataframe(infer_df, infer_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8845d961",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Eval Common Params\n",
    "######################################\n",
    "EVAL_COMMON_PARAMS = {}\n",
    "EVAL_COMMON_PARAMS[\"infer_filename\"] = INFER_COMMON_PARAMS[\"infer_filename\"]\n",
    "\n",
    "\n",
    "######################################\n",
    "# Eval Template\n",
    "######################################\n",
    "@rank_zero_only\n",
    "def run_eval(paths: dict, eval_common_params: dict) -> None:\n",
    "    infer_file = os.path.join(paths[\"inference_dir\"], eval_common_params[\"infer_filename\"])\n",
    "\n",
    "    fuse_logger_start(output_path=None, console_verbose_level=logging.INFO)\n",
    "    lgr = logging.getLogger(\"Fuse\")\n",
    "    lgr.info(\"Fuse Eval\", {\"attrs\": [\"bold\", \"underline\"]})\n",
    "\n",
    "    # metrics\n",
    "    metrics = OrderedDict(\n",
    "        [\n",
    "            (\"op\", MetricApplyThresholds(pred=\"model.output.head_0\")),  # will apply argmax\n",
    "            (\"auc\", MetricAUCROC(pred=\"model.output.head_0\", target=\"data.label\")),\n",
    "            (\"accuracy\", MetricAccuracy(pred=\"results:metrics.op.cls_pred\", target=\"data.label\")),\n",
    "            (\n",
    "                \"roc\",\n",
    "                MetricROCCurve(\n",
    "                    pred=\"model.output.head_0\",\n",
    "                    target=\"data.label\",\n",
    "                    output_filדיןאנאוקename=os.path.join(paths[\"inference_dir\"], \"roc_curve.png\"),\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # create evaluator\n",
    "    evaluator = EvaluatorDefault()\n",
    "\n",
    "    # run\n",
    "    results = evaluator.eval(ids=None, data=infer_file, metrics=metrics, output_dir=paths[\"eval_dir\"])\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c4ce05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISIC download \n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Run\n",
    "######################################\n",
    "if __name__ == \"__main__\":\n",
    "    ## allocate gpus\n",
    "    # uncomment if you want to use specific gpus instead of automatically looking for free ones\n",
    "    force_gpus = None  # [0]\n",
    "    ###Michal add the 3rd parameter use cpu if fail--> true\n",
    "    GPU.choose_and_enable_multiple_gpus(NUM_GPUS, force_gpus=force_gpus, use_cpu_if_fail=True)\n",
    "\n",
    "    ISIC.download(data_path=PATHS[\"data_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a744135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuse Train\n",
      "model_dir=./_examples/isic/model_dir_imaging\n",
      "data_dir=./_examples/isic/data_dir\n",
      "cache_dir=./_examples/isic/cache_dir\n",
      "Datamodule:\n",
      "before ISICDataModule\n",
      "ISIC dataset \n",
      "ISIC download \n",
      "ISIC static_pipeline \n",
      "in load dataframe file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_GroundTruth.csv\n",
      "in load dataframe file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Metadata.csv\n",
      "before printing static\n",
      "0@init_@call___call__@@<module 'fuseimg.datasets.isic' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuseimg\\\\datasets\\\\isic.py'>@    def __call__(self, sample_dict: NDict) -> NDict:\n",
      "        \"\"\"\n",
      "        decodes sample id into image file name\n",
      "        \"\"\"\n",
      "        sid = get_sample_id(sample_dict)\n",
      "        print (\"in ISIC OpISICSampleIDDecode _call_ sid= \")\n",
      "        img_filename_key = \"data.input.img_path\"\n",
      "        sample_dict[img_filename_key] = sid + \".jpg\"\n",
      "\n",
      "        return sample_dict\n",
      "@{}@1@init_@call___call__@format@infer@key_metadata_out@None@<module 'fuseimg.data.ops.image_loader' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuseimg\\\\data\\\\ops\\\\image_loader.py'>@    def __call__(\n",
      "        self,\n",
      "        sample_dict: NDict,\n",
      "        op_id: Optional[str],\n",
      "        key_in: str,\n",
      "        key_out: str,\n",
      "        key_metadata_out: Optional[str] = None,\n",
      "        format: str = \"infer\",\n",
      "    ):\n",
      "        \"\"\"\n",
      "        :param key_in: the key name in sample_dict that holds the filename\n",
      "        :param key_out: the key name in sample_dict that holds the image\n",
      "        :param key_metadata_out : the key to hold metadata dictionary\n",
      "        \"\"\"\n",
      "        img_filename = os.path.join(self._dir_path, sample_dict[key_in])\n",
      "        img_filename_suffix = img_filename.split(\".\")[-1]\n",
      "        print (\"OpLoadImage, img file: \",img_filename,img_filename_suffix)\n",
      "        if (format == \"infer\" and img_filename_suffix in [\"nii\"]) or (format in [\"nii\", \"nib\"]):\n",
      "\n",
      "            img = nib.load(img_filename)\n",
      "            img_np = img.get_fdata()\n",
      "            sample_dict[key_out] = img_np\n",
      "\n",
      "        elif img_filename_suffix in [\"jpg\", \"jpeg\", \"png\"]:\n",
      "            img = read_image(img_filename)\n",
      "            img = img.float()\n",
      "            img_np = img.numpy()\n",
      "            sample_dict[key_out] = img_np\n",
      "\n",
      "        elif (format == \"infer\" and img_filename_suffix in [\"mha\"]) or (format in [\"mha\"]):\n",
      "            image_data, image_header = load(img_filename)\n",
      "            sample_dict[key_out] = image_data\n",
      "            if key_metadata_out is not None:\n",
      "                sample_dict[key_metadata_out] = {\n",
      "                    key: image_header.sitkimage.GetMetaData(key) for key in image_header.sitkimage.GetMetaDataKeys()\n",
      "                }\n",
      "        elif (format == \"infer\" and img_filename_suffix in [\"dcm\"]) or (format in [\"dcm\"]):\n",
      "            dcm = pydicom.dcmread(img_filename)\n",
      "            inner_image = dcm.pixel_array\n",
      "            # convert to numpy\n",
      "            img_np = np.asarray(inner_image)\n",
      "            sample_dict[key_out] = img_np\n",
      "            if key_metadata_out is not None:\n",
      "                metadata = NDict()\n",
      "                for key, field in key_metadata_out:\n",
      "                    metadata[field] = dcm[key].value\n",
      "                sample_dict[key_metadata_out] = metadata\n",
      "        else:\n",
      "            raise Exception(\n",
      "                f\"OpLoadImage: case format {format} and {img_filename_suffix} is not supported - filename {img_filename}\"\n",
      "            )\n",
      "            \n",
      "        from PIL import Image\n",
      "\n",
      "#         w, h = 512, 512\n",
      "#         data = np.zeros((h, w, 3), dtype=np.uint8)\n",
      "#         data[0:256, 0:256] = [255, 0, 0] # red patch in upper left\n",
      "        img = Image.fromarray(img_np, 'RGB')\n",
      "#         img.save('my.png')\n",
      "        img.show()\n",
      "\n",
      "        return sample_dict\n",
      "@{'key_in': 'data.input.img_path', 'key_out': 'data.input.img'}@2@init_@call___call__@@<module 'fuseimg.data.ops.color' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuseimg\\\\data\\\\ops\\\\color.py'>@    def __call__(\n",
      "        self,\n",
      "        sample_dict: NDict,\n",
      "        key: str,\n",
      "        from_range: Tuple[float, float],\n",
      "        to_range: Tuple[float, float],\n",
      "    ):\n",
      "\n",
      "        from_range_start = from_range[0]\n",
      "        from_range_end = from_range[1]\n",
      "        to_range_start = to_range[0]\n",
      "        to_range_end = to_range[1]\n",
      "\n",
      "        img = sample_dict[key]\n",
      "\n",
      "        # shift to start at 0\n",
      "        img -= from_range_start\n",
      "\n",
      "        # scale to be in desired range\n",
      "        img *= (to_range_end - to_range_start) / (from_range_end - from_range_start)\n",
      "        # shift to start in desired start val\n",
      "        img += to_range_start\n",
      "\n",
      "        sample_dict[key] = img\n",
      "\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.img', 'from_range': (0, 255), 'to_range': (0, 1)}@3@init_@call___call__@prefix@None@<module 'fuse.data.ops.ops_read' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_read.py'>@    def __call__(self, sample_dict: NDict, prefix: Optional[str] = None) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        See base class\n",
      "\n",
      "        :param prefix: specify a prefix for the sample dict keys.\n",
      "                       For example, with prefix 'data.features' and a df with the columns ['height', 'weight', 'sex'],\n",
      "                       the matching keys will be: 'data.features.height', 'data.features.weight', 'data.features.sex'.\n",
      "        \"\"\"\n",
      "        key = sample_dict[self._key_name]\n",
      "\n",
      "        # locate the required item\n",
      "        sample_data = self._data[key].copy()\n",
      "\n",
      "        # add values tp sample_dict\n",
      "        for name, value in sample_data.items():\n",
      "            if prefix is None:\n",
      "                sample_dict[name] = value\n",
      "            else:\n",
      "                sample_dict[f\"{prefix}.{name}\"] = value\n",
      "\n",
      "        return sample_dict\n",
      "@{}@4@init_@call___call__@prefix@None@<module 'fuse.data.ops.ops_read' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_read.py'>@    def __call__(self, sample_dict: NDict, prefix: Optional[str] = None) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        See base class\n",
      "\n",
      "        :param prefix: specify a prefix for the sample dict keys.\n",
      "                       For example, with prefix 'data.features' and a df with the columns ['height', 'weight', 'sex'],\n",
      "                       the matching keys will be: 'data.features.height', 'data.features.weight', 'data.features.sex'.\n",
      "        \"\"\"\n",
      "        key = sample_dict[self._key_name]\n",
      "\n",
      "        # locate the required item\n",
      "        sample_data = self._data[key].copy()\n",
      "\n",
      "        # add values tp sample_dict\n",
      "        for name, value in sample_data.items():\n",
      "            if prefix is None:\n",
      "                sample_dict[name] = value\n",
      "            else:\n",
      "                sample_dict[f\"{prefix}.{name}\"] = value\n",
      "\n",
      "        return sample_dict\n",
      "@{}@5@init_@call___call__@@<module 'fuse.data.ops.ops_common' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_common.py'>@    def __call__(self, sample_dict: NDict, key: str, value_to_fill: Any) -> NDict:\n",
      "        assert key in sample_dict, f\"Error: missing {key}, available keys {sample_dict.keypaths()} \"\n",
      "        if isinstance(sample_dict[key], numbers.Number) and math.isnan(sample_dict[key]):\n",
      "            sample_dict[key] = value_to_fill\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.clinical.anatom_site_general', 'value_to_fill': 'N/A'}@6@init_@call___call__@@<module 'fuse.data.ops.ops_common' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_common.py'>@    def __call__(self, sample_dict: NDict, key: str, value_to_fill: Any) -> NDict:\n",
      "        assert key in sample_dict, f\"Error: missing {key}, available keys {sample_dict.keypaths()} \"\n",
      "        if isinstance(sample_dict[key], numbers.Number) and math.isnan(sample_dict[key]):\n",
      "            sample_dict[key] = value_to_fill\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.clinical.sex', 'value_to_fill': 'N/A'}@7@init_@call___call__@@<module 'fuse.data.ops.ops_common' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_common.py'>@    def __call__(self, sample_dict: NDict, key: str, value_to_fill: Any) -> NDict:\n",
      "        assert key in sample_dict, f\"Error: missing {key}, available keys {sample_dict.keypaths()} \"\n",
      "        if isinstance(sample_dict[key], numbers.Number) and math.isnan(sample_dict[key]):\n",
      "            sample_dict[key] = value_to_fill\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.clinical.age_approx', 'value_to_fill': -1.0}@8@init_@call___call__@key@None@<module 'fuse.data.ops.ops_common' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_common.py'>@    def __call__(\n",
      "        self, sample_dict: NDict, op_id: Optional[str], key: Optional[str] = None, **kwargs\n",
      "    ) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        More details in super class\n",
      "        :param key: apply lambda func on sample_dict[key]. If none the input and output of the lambda function are the entire sample_dict\n",
      "        \"\"\"\n",
      "        sample_dict[op_id] = key\n",
      "        if key is not None:\n",
      "            value = sample_dict[key]\n",
      "            value = self._func(value, **kwargs)\n",
      "            sample_dict[key] = value\n",
      "        else:\n",
      "            sample_dict = self._func(sample_dict)\n",
      "\n",
      "        return sample_dict\n",
      "@{}@9@init_@call___call__@@<module 'fuseimg.datasets.isic' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuseimg\\\\datasets\\\\isic.py'>@    def __call__(self, sample_dict: NDict, key_site: str, key_sex: str, key_age: str, out_prefix: str) -> NDict:\n",
      "        \"\"\"\n",
      "        :param key_site: sample_dict's key for patient's anatom site data\n",
      "        :param key_sex: sample_dict's key for patient's sex data\n",
      "        :param key_age: sample_dict's key for patient's age data\n",
      "        :param out_prefix: the encoded data will be located in sample_dict[f\"{out_prefix}.{data_type}\"]\n",
      "        \"\"\"\n",
      "\n",
      "        # Encode anatom site into a one-hot vector of length 9\n",
      "        # 8 anatom sites and 1 for N/A\n",
      "        if \"site\" in self._items_to_encode:\n",
      "            site = sample_dict[key_site]\n",
      "            site_one_hot = np.zeros(len(ISIC.ANATOM_SITE_INDEX))\n",
      "            if site in ISIC.ANATOM_SITE_INDEX:\n",
      "                site_one_hot[ISIC.ANATOM_SITE_INDEX[site]] = 1\n",
      "\n",
      "            sample_dict[f\"{out_prefix}.site\"] = site_one_hot\n",
      "\n",
      "        # Encode sex into a one-hot vector of length 3\n",
      "        # male, female, N/A\n",
      "        if \"sex\" in self._items_to_encode:\n",
      "            sex = sample_dict[key_sex]\n",
      "            sex_one_hot = np.zeros(len(ISIC.SEX_INDEX))\n",
      "            if sex in ISIC.SEX_INDEX:\n",
      "                sex_one_hot[ISIC.SEX_INDEX[sex]] = 1\n",
      "\n",
      "            sample_dict[f\"{out_prefix}.sex\"] = sex_one_hot\n",
      "\n",
      "        # Encode age into one-hot vector 'u' with length 7 such that:\n",
      "        # for i in (0, ..., 5), u[i] == 1 iff age in range (20*i, 20(i+1)),\n",
      "        # and u[6] == 1 iff age has missing value (0> or 120<)\n",
      "        # for examples:\n",
      "        #   age 50 -> [0,0,1,0,0,0,0]\n",
      "        #   age 90 -> [0,0,0,0,1,0,0]\n",
      "        #   missing age -> [0,0,0,0,0,0,1]\n",
      "        if \"age\" in self._items_to_encode:\n",
      "            age = int(sample_dict[key_age])\n",
      "            age_one_hot = np.zeros(7)\n",
      "\n",
      "            encode_idx = age // 20 if (age > 0 and age < 120) else 6\n",
      "            age_one_hot[encode_idx] = 1\n",
      "\n",
      "            sample_dict[f\"{out_prefix}.age\"] = age_one_hot\n",
      "\n",
      "        return sample_dict\n",
      "@{'key_site': 'data.input.clinical.anatom_site_general', 'key_sex': 'data.input.clinical.sex', 'key_age': 'data.input.clinical.age_approx', 'out_prefix': 'data.input.clinical.encoding'}@\n",
      "ISIC dynamic_pipeline \n",
      "before printing dynamic\n",
      "0@init_@call___call__@@<module 'fuseimg.data.ops.aug.geometry' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuseimg\\\\data\\\\ops\\\\aug\\\\geometry.py'>@    def __call__(self, sample_dict: NDict, output_shape: Tuple[int], key: str, **kwargs) -> NDict:\n",
      "        \"\"\"\n",
      "        :param key: key to a numpy array or tensor stored in the sample_dict in a H x W x C format.\n",
      "        :param kwargs: additional arguments to pass to the resize function\n",
      "\n",
      "        Stores the resized image in sample_dict[key]\n",
      "        \"\"\"\n",
      "        aug_input = sample_dict[key]\n",
      "        dim = len(aug_input.shape)\n",
      "\n",
      "        if self._channels_first:\n",
      "            # Permutes CxHxW -> HxWxC (for skimage's resize)\n",
      "            perm = self.get_permutation(dim=dim, channels_first=True)\n",
      "            aug_input = np.transpose(aug_input, axes=perm)\n",
      "\n",
      "        # Apply Resize\n",
      "        aug_output = skimage.transform.resize(image=aug_input, output_shape=output_shape, **kwargs)\n",
      "\n",
      "        if self._channels_first:\n",
      "            # Permutes back HxWxC -> CxHxW\n",
      "            perm = self.get_permutation(dim=dim, channels_first=False)\n",
      "            aug_output = np.transpose(aug_output, axes=perm)\n",
      "\n",
      "        sample_dict[key] = aug_output\n",
      "\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.img', 'output_shape': (300, 300, 3), 'mode': 'reflect', 'anti_aliasing': True}@1@init_@call___call__@@<module 'fuse.data.ops.ops_cast' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_cast.py'>@    def __call__(\n",
      "        self, sample_dict: NDict, op_id: Optional[str], key: Union[str, Sequence[str]], **kwargs\n",
      "    ) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        See super class\n",
      "        :param key: single key or list of keys from sample_dict to convert\n",
      "        \"\"\"\n",
      "        if isinstance(key, str):\n",
      "            keys = [key]\n",
      "        else:\n",
      "            keys = key\n",
      "\n",
      "        for key_name in keys:\n",
      "            value = sample_dict[key_name]\n",
      "            sample_dict[f\"{op_id}_{key_name}\"] = type(value).__name__\n",
      "            value = self._cast(value, **kwargs)\n",
      "            sample_dict[key_name] = value\n",
      "\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.img', 'dtype': torch.float32}@2@init_@call___call__@axis@0@<module 'fuse.data.ops.ops_common' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_common.py'>@    def __call__(\n",
      "        self, sample_dict: NDict, keys_in: Sequence[str], key_out: str, axis: int = 0\n",
      "    ) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        :param keys_in: sequence of keys to numpy arrays we want to concatenate\n",
      "        :param key_out: the key to store the concatenated vector\n",
      "        :param axis: concatenate along the specified axis\n",
      "        \"\"\"\n",
      "        values = [np.asarray(sample_dict[key_in]) for key_in in keys_in]\n",
      "        values = [v if len(v.shape) > 0 else np.expand_dims(v, axis=0) for v in values]\n",
      "        sample_dict[key_out] = np.concatenate(values, axis=axis)\n",
      "\n",
      "        return sample_dict\n",
      "@{'keys_in': ['data.input.clinical.encoding.site', 'data.input.clinical.encoding.sex', 'data.input.clinical.encoding.age'], 'key_out': 'data.input.clinical.all'}@3@init_@call___call__@@<module 'fuse.data.ops.ops_cast' from 'C:\\\\Users\\\\USER\\\\Documents\\\\fuse-med-ml-master-1\\\\fuse\\\\data\\\\ops\\\\ops_cast.py'>@    def __call__(\n",
      "        self, sample_dict: NDict, op_id: Optional[str], key: Union[str, Sequence[str]], **kwargs\n",
      "    ) -> Union[None, dict, List[dict]]:\n",
      "        \"\"\"\n",
      "        See super class\n",
      "        :param key: single key or list of keys from sample_dict to convert\n",
      "        \"\"\"\n",
      "        if isinstance(key, str):\n",
      "            keys = [key]\n",
      "        else:\n",
      "            keys = key\n",
      "\n",
      "        for key_name in keys:\n",
      "            value = sample_dict[key_name]\n",
      "            sample_dict[f\"{op_id}_{key_name}\"] = type(value).__name__\n",
      "            value = self._cast(value, **kwargs)\n",
      "            sample_dict[key_name] = value\n",
      "\n",
      "        return sample_dict\n",
      "@{'key': 'data.input.clinical.all', 'dtype': torch.float32}@\n",
      "pipeline description hash for [isic_cache_ver0] is: hash_ba80db34b3fed6fa5a43284ddb7769b2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\Documents\\fuse-med-ml-master-1\\fuse\\data\\datasets\\caching\\samples_cacher.py:110: UserWarning: Multi processing is not active in SamplesCacher. Seting \"workers\" to the number of your cores usually results in a significant speedup. Debugging, however, is easier with \"workers=0\".\n",
      "  warn(\n",
      "caching:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072637.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  10%|█         | 1/10 [00:03<00:32,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@bf35a2a43e5797c123fd45586d96b83a.hdf5f9739b6fd66a90f7ac546dacd975af6c.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@bf35a2a43e5797c123fd45586d96b83a.pkl.gzfb6dee5f764bb0ecd5e9ed869f4402c0.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@bf35a2a43e5797c123fd45586d96b83a.pkla5c6d390a58181d22362c1169c84a19e.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072638.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  20%|██        | 2/10 [00:06<00:27,  3.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@7eb09566db528503a1a8d36d8e8d9aaa.hdf543285c810d1ed808fd259f6a54159e10.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@7eb09566db528503a1a8d36d8e8d9aaa.pkl.gzd32a5ff69aca37d43c57de0670a4391b.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@7eb09566db528503a1a8d36d8e8d9aaa.pkl1807eecd3aba0ddaabf11237c328022b.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072639.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  30%|███       | 3/10 [00:10<00:23,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@64c21ca4aafe3ab7aaec18c965acf170.hdf5ffe539aa60867b480f8bcbeaedd03c3d.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@64c21ca4aafe3ab7aaec18c965acf170.pkl.gz218e041c389b03dfdce391aeddf25923.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@64c21ca4aafe3ab7aaec18c965acf170.pkl6adc8e6cff1f9740e90ac5bb2d3fc831.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072640.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  40%|████      | 4/10 [00:13<00:20,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@e14a55f1bff588b8a32118599145d2c9.hdf508ab00d26420217ee5d5e7da981a8a19.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@e14a55f1bff588b8a32118599145d2c9.pkl.gz8db9561891429933a62de8dc2ca86b5b.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@e14a55f1bff588b8a32118599145d2c9.pkle2cc7be54ef0bd196dae28f222a0cb24.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072641.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  50%|█████     | 5/10 [00:16<00:16,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@4b676a1499500b2bdf2a4beb03fbe1c3.hdf566a2dfa56e1a3eee202971d37629589b.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@4b676a1499500b2bdf2a4beb03fbe1c3.pkl.gz7e62c43f65e41b42f8f8cca480cc1183.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@4b676a1499500b2bdf2a4beb03fbe1c3.pklbce5d5c5b137f8e98c053a1fd2a2c37f.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072642.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  60%|██████    | 6/10 [00:20<00:13,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@84e81d388bbacfca2798d74295b39209.hdf56cdd35cbcea6e0cf2f53e14b907d4b1d.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@84e81d388bbacfca2798d74295b39209.pkl.gz85062cd79de4f6789f7b76114f0272a9.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@84e81d388bbacfca2798d74295b39209.pklabe2083daba1ef3838b6a12673fc33c8.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072646.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  70%|███████   | 7/10 [00:23<00:10,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@e655958d1950481e4d8403876c9dffcf.hdf5fa7f86a028612c681f1e54bb7bc7415b.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@e655958d1950481e4d8403876c9dffcf.pkl.gzb74f107b559062e35ff85ba248f3e68e.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@e655958d1950481e4d8403876c9dffcf.pkl1b6da6f575d4e3d9a6edb8ffa3763beb.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072647.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  80%|████████  | 8/10 [00:26<00:06,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@726d84ed4e7280039be9c0e8ee30e9b0.hdf52648a4f93720d3dd7e6f905f6a2faf4f.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@726d84ed4e7280039be9c0e8ee30e9b0.pkl.gzd30d071c8a3e95bfd5aced929ca1d2c8.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@726d84ed4e7280039be9c0e8ee30e9b0.pkl98da685df793d1bd97155fd1cc83e4a2.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072648.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "caching:  90%|█████████ | 9/10 [00:30<00:03,  3.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@6eee2b91b0abe3870a5a6038b7b94514.hdf5a8fe5bc2f09b1eaa7c7581c05c7d4ce5.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@6eee2b91b0abe3870a5a6038b7b94514.pkl.gzadffc8ddb3f58aff43a8f9c2514e41f2.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@6eee2b91b0abe3870a5a6038b7b94514.pklfc6322379c3f20adbe388878ba091d80.scramlbed\n",
      "in ISIC OpISICSampleIDDecode _call_ sid= \n",
      "OpLoadImage, img file:  ./_examples/isic/data_dir\\ISIC2019/ISIC_2019_Training_Input\\ISIC_0072649.jpg jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "caching: 100%|██████████| 10/10 [00:33<00:00,  3.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in save safe hdf5 file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@9eb861eb3cc48fe29c0a849ea9665c0d.hdf520b41d6c190a374258ec512bb1e9b19a.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_sample_id@9eb861eb3cc48fe29c0a849ea9665c0d.pkl.gz5dfc19a75e764c9919855a1d5af33742.scramlbed\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\out_info_for_orig_sample@9eb861eb3cc48fe29c0a849ea9665c0d.pkl2f9a4545c2d2e1e8c7726482fcf8fec8.scramlbed\n",
      "======== wrote ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\pipeline_hash_ba80db34b3fed6fa5a43284ddb7769b2_desc.txt\n",
      "in save pickle file:  ./_examples/isic/cache_dir\\isic_cache_ver0\\hash_ba80db34b3fed6fa5a43284ddb7769b2\\full_sets_info\\samples_ids_hash@437650e084bf7640c0f0882589810bf9.pkl.gze335322e163d087a96db613655fca950.scramlbed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "    RUNNING_MODES = [\"train\", \"infer\", \"eval\"]  # Options: 'train', 'infer', 'eval'\n",
    "\n",
    "    # train\n",
    "    if \"train\" in RUNNING_MODES:\n",
    "        run_train(paths=PATHS, train_common_params=TRAIN_COMMON_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a96b4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # infer\n",
    "    if \"infer\" in RUNNING_MODES:\n",
    "        run_infer(paths=PATHS, infer_common_params=INFER_COMMON_PARAMS)\n",
    "\n",
    "    # eval\n",
    "    if \"eval\" in RUNNING_MODES:\n",
    "        run_eval(paths=PATHS, eval_common_params=EVAL_COMMON_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52de37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edadd66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5536af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989064d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae13559b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d2f683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
